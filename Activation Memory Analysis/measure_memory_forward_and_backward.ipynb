{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ref: https://www.youtube.com/watch?v=-2ebSQROew4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/NS/llm-1/nobackup/afkhan/anaconda3/envs/fsdp_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch, gc, inspect, transformers\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from accelerate.utils import set_seed\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers.logging.set_verbosity_warning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measuring Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup():\n",
    "    \"Free up memory and reset stats\"\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats(device)\n",
    "cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max memory allocated: 0.00 GB\n",
      "Max memory reserved: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "def print_memory_stats():\n",
    "    \"\"\"Print two different measures of GPU memory usage\"\"\"\n",
    "    max_mem_allocated = torch.cuda.max_memory_allocated(device)/1e9\n",
    "    max_mem_reserved = torch.cuda.max_memory_reserved(device)/1e9\n",
    "    print(f\"Max memory allocated: {max_mem_allocated:.2f} GB\")\n",
    "    # reserved (aka 'max_memory_cached') is ~the allocated memory plus pre-cached memory\n",
    "    print(f\"Max memory reserved: {max_mem_reserved:.2f} GB\")\n",
    "\n",
    "    return max_mem_allocated, max_mem_reserved\n",
    "\n",
    "max_mem_allocated, max_mem_reserved = print_memory_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleanup()\n",
    "\n",
    "# model_path = '/NS/llm-1/nobackup/vnanda/llm_base_models/pythia-1b'\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_path).to(device)\n",
    "\n",
    "# print(\"Model loaded\")\n",
    "\n",
    "# batch_size = 1\n",
    "# context_length = 1024\n",
    "\n",
    "# data = torch.randint(0, 10000, (batch_size, context_length), device=device)\n",
    "\n",
    "# output = model(data, labels=data)\n",
    "\n",
    "# output.loss.backward()\n",
    "\n",
    "# # Print memory stats\n",
    "# print_memory_stats()\n",
    "\n",
    "# # Cleanup\n",
    "# del model, data, output\n",
    "# cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '/NS/llm-1/nobackup/vnanda/llm_base_models/pythia-12b'\n",
    "MODEL = model_path.split('/')[-1]\n",
    "measurements = {}\n",
    "BATCH_SIZES = [1,2,4,6,8]\n",
    "CONTEXT_LENGTHS = [1,2,4,6,8,16,32,64,128,256,512,1024,2048]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['memory_measurements_pythia-1b.json', 'memory_measurements_pythia-70m.json', 'memory_measurements_pythia-6.9b.json']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "available_measurements = os.listdir('Measurements')\n",
    "\n",
    "print(available_measurements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 1, Context length: 1 already measured\n",
      "Max Memory Allocated: 100 GB\n",
      "Max Memory Reserved: 100 GB\n",
      "Batch size: 1, Context length: 2 already measured\n",
      "Max Memory Allocated: 100 GB\n",
      "Max Memory Reserved: 100 GB\n",
      "Batch size: 1, Context length: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 1, Context length: 4 failed\n",
      "CUDA out of memory. Tried to allocate 990.00 MiB. GPU \n",
      "Batch size: 1, Context length: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Batch size: 1, Context length: 6 failed\n",
      "CUDA out of memory. Tried to allocate 400.00 MiB. GPU \n",
      "Batch size: 1, Context length: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Batch size: 1, Context length: 8 failed\n",
      "CUDA out of memory. Tried to allocate 400.00 MiB. GPU \n",
      "Batch size: 1, Context length: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Batch size: 1, Context length: 16 failed\n",
      "CUDA out of memory. Tried to allocate 400.00 MiB. GPU \n",
      "Batch size: 1, Context length: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Batch size: 1, Context length: 32 failed\n",
      "CUDA out of memory. Tried to allocate 400.00 MiB. GPU \n",
      "Batch size: 1, Context length: 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Batch size: 1, Context length: 64 failed\n",
      "CUDA out of memory. Tried to allocate 300.00 MiB. GPU \n",
      "Batch size: 1, Context length: 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Batch size: 1, Context length: 128 failed\n",
      "CUDA out of memory. Tried to allocate 400.00 MiB. GPU \n",
      "Batch size: 1, Context length: 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Batch size: 1, Context length: 256 failed\n",
      "CUDA out of memory. Tried to allocate 300.00 MiB. GPU \n",
      "Batch size: 1, Context length: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Batch size: 1, Context length: 512 failed\n",
      "CUDA out of memory. Tried to allocate 300.00 MiB. GPU \n",
      "Batch size: 1, Context length: 1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Batch size: 1, Context length: 1024 failed\n",
      "CUDA out of memory. Tried to allocate 400.00 MiB. GPU \n",
      "Batch size: 1, Context length: 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Batch size: 1, Context length: 2048 failed\n",
      "CUDA out of memory. Tried to allocate 120.00 MiB. GPU \n",
      "Batch size: 2, Context length: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Batch size: 2, Context length: 1 failed\n",
      "CUDA out of memory. Tried to allocate 400.00 MiB. GPU \n",
      "Batch size: 2, Context length: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Batch size: 2, Context length: 2 failed\n",
      "CUDA out of memory. Tried to allocate 400.00 MiB. GPU \n",
      "Batch size: 2, Context length: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Batch size: 2, Context length: 4 failed\n",
      "CUDA out of memory. Tried to allocate 400.00 MiB. GPU \n",
      "Batch size: 2, Context length: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Batch size: 2, Context length: 6 failed\n",
      "CUDA out of memory. Tried to allocate 400.00 MiB. GPU \n",
      "Batch size: 2, Context length: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Batch size: 2, Context length: 8 failed\n",
      "CUDA out of memory. Tried to allocate 400.00 MiB. GPU \n",
      "Batch size: 2, Context length: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Batch size: 2, Context length: 16 failed\n",
      "CUDA out of memory. Tried to allocate 400.00 MiB. GPU \n",
      "Batch size: 2, Context length: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Batch size: 2, Context length: 32 failed\n",
      "CUDA out of memory. Tried to allocate 300.00 MiB. GPU \n",
      "Batch size: 2, Context length: 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Batch size: 2, Context length: 64 failed\n",
      "CUDA out of memory. Tried to allocate 400.00 MiB. GPU \n",
      "Batch size: 2, Context length: 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Batch size: 2, Context length: 128 failed\n",
      "CUDA out of memory. Tried to allocate 300.00 MiB. GPU \n",
      "Batch size: 2, Context length: 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Batch size: 2, Context length: 256 failed\n",
      "CUDA out of memory. Tried to allocate 300.00 MiB. GPU \n",
      "Batch size: 2, Context length: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Batch size: 2, Context length: 512 failed\n",
      "CUDA out of memory. Tried to allocate 400.00 MiB. GPU \n",
      "Batch size: 2, Context length: 1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Batch size: 2, Context length: 1024 failed\n",
      "CUDA out of memory. Tried to allocate 120.00 MiB. GPU \n",
      "Batch size: 2, Context length: 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Batch size: 2, Context length: 2048 failed\n",
      "CUDA out of memory. Tried to allocate 240.00 MiB. GPU \n",
      "Batch size: 4, Context length: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Batch size: 4, Context length: 1 failed\n",
      "CUDA out of memory. Tried to allocate 400.00 MiB. GPU \n",
      "Batch size: 4, Context length: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Batch size: 4, Context length: 2 failed\n",
      "CUDA out of memory. Tried to allocate 400.00 MiB. GPU \n",
      "Batch size: 4, Context length: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Batch size: 4, Context length: 4 failed\n",
      "CUDA out of memory. Tried to allocate 400.00 MiB. GPU \n",
      "Batch size: 4, Context length: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Batch size: 4, Context length: 6 failed\n",
      "CUDA out of memory. Tried to allocate 400.00 MiB. GPU \n",
      "Batch size: 4, Context length: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Batch size: 4, Context length: 8 failed\n",
      "CUDA out of memory. Tried to allocate 400.00 MiB. GPU \n",
      "Batch size: 4, Context length: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Batch size: 4, Context length: 16 failed\n",
      "CUDA out of memory. Tried to allocate 300.00 MiB. GPU \n",
      "Batch size: 4, Context length: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Batch size: 4, Context length: 32 failed\n",
      "CUDA out of memory. Tried to allocate 400.00 MiB. GPU \n",
      "Batch size: 4, Context length: 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Batch size: 4, Context length: 64 failed\n",
      "CUDA out of memory. Tried to allocate 300.00 MiB. GPU \n",
      "Batch size: 4, Context length: 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Batch size: 4, Context length: 128 failed\n",
      "CUDA out of memory. Tried to allocate 300.00 MiB. GPU \n",
      "Batch size: 4, Context length: 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Batch size: 4, Context length: 256 failed\n",
      "CUDA out of memory. Tried to allocate 400.00 MiB. GPU \n",
      "Batch size: 4, Context length: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Batch size: 4, Context length: 512 failed\n",
      "CUDA out of memory. Tried to allocate 120.00 MiB. GPU \n",
      "Batch size: 4, Context length: 1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Batch size: 4, Context length: 1024 failed\n",
      "CUDA out of memory. Tried to allocate 240.00 MiB. GPU \n",
      "Batch size: 4, Context length: 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Batch size: 4, Context length: 2048 failed\n",
      "CUDA out of memory. Tried to allocate 640.00 MiB. GPU \n",
      "Batch size: 6, Context length: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Batch size: 6, Context length: 1 failed\n",
      "CUDA out of memory. Tried to allocate 400.00 MiB. GPU \n",
      "Batch size: 6, Context length: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Batch size: 6, Context length: 2 failed\n",
      "CUDA out of memory. Tried to allocate 400.00 MiB. GPU \n",
      "Batch size: 6, Context length: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Batch size: 6, Context length: 4 failed\n",
      "CUDA out of memory. Tried to allocate 400.00 MiB. GPU \n",
      "Batch size: 6, Context length: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Batch size: 6, Context length: 6 failed\n",
      "CUDA out of memory. Tried to allocate 400.00 MiB. GPU \n",
      "Batch size: 6, Context length: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Batch size: 6, Context length: 8 failed\n",
      "CUDA out of memory. Tried to allocate 400.00 MiB. GPU \n",
      "Batch size: 6, Context length: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Batch size: 6, Context length: 16 failed\n",
      "CUDA out of memory. Tried to allocate 100.00 MiB. GPU \n",
      "Batch size: 6, Context length: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Batch size: 6, Context length: 32 failed\n",
      "CUDA out of memory. Tried to allocate 400.00 MiB. GPU \n",
      "Batch size: 6, Context length: 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Batch size: 6, Context length: 64 failed\n",
      "CUDA out of memory. Tried to allocate 400.00 MiB. GPU \n",
      "Batch size: 6, Context length: 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Batch size: 6, Context length: 128 failed\n",
      "CUDA out of memory. Tried to allocate 300.00 MiB. GPU \n",
      "Batch size: 6, Context length: 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Batch size: 6, Context length: 256 failed\n",
      "CUDA out of memory. Tried to allocate 400.00 MiB. GPU \n",
      "Batch size: 6, Context length: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Batch size: 6, Context length: 512 failed\n",
      "CUDA out of memory. Tried to allocate 240.00 MiB. GPU \n",
      "Batch size: 6, Context length: 1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Batch size: 6, Context length: 1024 failed\n",
      "CUDA out of memory. Tried to allocate 120.00 MiB. GPU \n",
      "Batch size: 6, Context length: 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Batch size: 6, Context length: 2048 failed\n",
      "CUDA out of memory. Tried to allocate 960.00 MiB. GPU \n",
      "Batch size: 8, Context length: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Batch size: 8, Context length: 1 failed\n",
      "CUDA out of memory. Tried to allocate 400.00 MiB. GPU \n",
      "Batch size: 8, Context length: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Batch size: 8, Context length: 2 failed\n",
      "CUDA out of memory. Tried to allocate 400.00 MiB. GPU \n",
      "Batch size: 8, Context length: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Batch size: 8, Context length: 4 failed\n",
      "CUDA out of memory. Tried to allocate 400.00 MiB. GPU \n",
      "Batch size: 8, Context length: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Batch size: 8, Context length: 6 failed\n",
      "CUDA out of memory. Tried to allocate 400.00 MiB. GPU \n",
      "Batch size: 8, Context length: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Batch size: 8, Context length: 8 failed\n",
      "CUDA out of memory. Tried to allocate 300.00 MiB. GPU \n",
      "Batch size: 8, Context length: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Batch size: 8, Context length: 16 failed\n",
      "CUDA out of memory. Tried to allocate 400.00 MiB. GPU \n",
      "Batch size: 8, Context length: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Batch size: 8, Context length: 32 failed\n",
      "CUDA out of memory. Tried to allocate 300.00 MiB. GPU \n",
      "Batch size: 8, Context length: 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Batch size: 8, Context length: 64 failed\n",
      "CUDA out of memory. Tried to allocate 22.00 MiB. GPU \n",
      "Batch size: 8, Context length: 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Batch size: 8, Context length: 128 failed\n",
      "CUDA out of memory. Tried to allocate 400.00 MiB. GPU \n",
      "Batch size: 8, Context length: 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Batch size: 8, Context length: 256 failed\n",
      "CUDA out of memory. Tried to allocate 120.00 MiB. GPU \n",
      "Batch size: 8, Context length: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Batch size: 8, Context length: 512 failed\n",
      "CUDA out of memory. Tried to allocate 240.00 MiB. GPU \n",
      "Batch size: 8, Context length: 1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Batch size: 8, Context length: 1024 failed\n",
      "CUDA out of memory. Tried to allocate 640.00 MiB. GPU \n",
      "Batch size: 8, Context length: 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Batch size: 8, Context length: 2048 failed\n",
      "CUDA out of memory. Tried to allocate 1.25 GiB. GPU \n"
     ]
    }
   ],
   "source": [
    "if 'memory_measurements_' + MODEL + '.json' in available_measurements:\n",
    "    \n",
    "    with open('Measurements/memory_measurements_' + MODEL + '.json', 'r') as f:\n",
    "        measurements = json.load(f)\n",
    "\n",
    "    print(\"Loaded measurements\")\n",
    "    \n",
    "    keys = list(measurements.keys())\n",
    "\n",
    "    # check if all batch sizes are present\n",
    "    for bs in BATCH_SIZES:\n",
    "        if bs not in keys:\n",
    "            print(f\"Batch size {bs} not present\")\n",
    "            continue\n",
    "        for cl in CONTEXT_LENGTHS:\n",
    "            if cl not in measurements[bs]:\n",
    "                print(f\"Context length {cl} not present for batch size {bs}\")\n",
    "                continue\n",
    "\n",
    "else:\n",
    "    for batch_size in BATCH_SIZES:\n",
    "        measurements[batch_size] = {} if batch_size not in measurements else measurements[batch_size]\n",
    "        for context_length in CONTEXT_LENGTHS:\n",
    "\n",
    "            if context_length in measurements[batch_size]:\n",
    "                print(f\"Batch size: {batch_size}, Context length: {context_length} already measured\")\n",
    "                print(f\"Max Memory Allocated: {measurements[batch_size][context_length][0]} GB\")\n",
    "                print(f\"Max Memory Reserved: {measurements[batch_size][context_length][1]} GB\")\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                print(f\"Batch size: {batch_size}, Context length: {context_length}\")\n",
    "                \n",
    "                cleanup()\n",
    "\n",
    "                model = AutoModelForCausalLM.from_pretrained(model_path).to(device)\n",
    "\n",
    "                print(\"Model loaded\")\n",
    "\n",
    "                data = torch.randint(0, 10000, (batch_size, context_length), device=device)\n",
    "\n",
    "                output = model(data, labels=data)\n",
    "\n",
    "                output.loss.backward()\n",
    "\n",
    "                # Print memory stats\n",
    "                max_mem_allocated, max_mem_reserved = print_memory_stats()\n",
    "\n",
    "                measurements[batch_size][context_length] = (max_mem_allocated, max_mem_reserved)\n",
    "\n",
    "                # Cleanup\n",
    "                del model, data, output\n",
    "                cleanup()\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Batch size: {batch_size}, Context length: {context_length} failed\")\n",
    "                print(e)\n",
    "                measurements[batch_size][context_length] = (100, 100)\n",
    "                try:\n",
    "                    del model\n",
    "                except:\n",
    "                    pass\n",
    "                try:\n",
    "                    del data\n",
    "                except:\n",
    "                    pass\n",
    "                try:\n",
    "                    del output\n",
    "                except:\n",
    "                    pass\n",
    "                cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: {1: (100, 100),\n",
       "  2: (100, 100),\n",
       "  4: (100, 100),\n",
       "  6: (100, 100),\n",
       "  8: (100, 100),\n",
       "  16: (100, 100),\n",
       "  32: (100, 100),\n",
       "  64: (100, 100),\n",
       "  128: (100, 100),\n",
       "  256: (100, 100),\n",
       "  512: (100, 100),\n",
       "  1024: (100, 100),\n",
       "  2048: (100, 100)},\n",
       " 2: {1: (100, 100),\n",
       "  2: (100, 100),\n",
       "  4: (100, 100),\n",
       "  6: (100, 100),\n",
       "  8: (100, 100),\n",
       "  16: (100, 100),\n",
       "  32: (100, 100),\n",
       "  64: (100, 100),\n",
       "  128: (100, 100),\n",
       "  256: (100, 100),\n",
       "  512: (100, 100),\n",
       "  1024: (100, 100),\n",
       "  2048: (100, 100)},\n",
       " 4: {1: (100, 100),\n",
       "  2: (100, 100),\n",
       "  4: (100, 100),\n",
       "  6: (100, 100),\n",
       "  8: (100, 100),\n",
       "  16: (100, 100),\n",
       "  32: (100, 100),\n",
       "  64: (100, 100),\n",
       "  128: (100, 100),\n",
       "  256: (100, 100),\n",
       "  512: (100, 100),\n",
       "  1024: (100, 100),\n",
       "  2048: (100, 100)},\n",
       " 6: {1: (100, 100),\n",
       "  2: (100, 100),\n",
       "  4: (100, 100),\n",
       "  6: (100, 100),\n",
       "  8: (100, 100),\n",
       "  16: (100, 100),\n",
       "  32: (100, 100),\n",
       "  64: (100, 100),\n",
       "  128: (100, 100),\n",
       "  256: (100, 100),\n",
       "  512: (100, 100),\n",
       "  1024: (100, 100),\n",
       "  2048: (100, 100)},\n",
       " 8: {1: (100, 100),\n",
       "  2: (100, 100),\n",
       "  4: (100, 100),\n",
       "  6: (100, 100),\n",
       "  8: (100, 100),\n",
       "  16: (100, 100),\n",
       "  32: (100, 100),\n",
       "  64: (100, 100),\n",
       "  128: (100, 100),\n",
       "  256: (100, 100),\n",
       "  512: (100, 100),\n",
       "  1024: (100, 100),\n",
       "  2048: (100, 100)}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump measurements\n",
    "import json\n",
    "\n",
    "with open(f\"Measurements/memory_measurements_{MODEL}.json\", 'w') as f:\n",
    "    json.dump(measurements, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
