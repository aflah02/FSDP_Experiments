{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ref: https://www.youtube.com/watch?v=-2ebSQROew4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc, inspect, transformers\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from accelerate.utils import set_seed\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers.logging.set_verbosity_warning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set visible device to 1\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measuring Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup():\n",
    "    \"Free up memory and reset stats\"\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats(device)\n",
    "cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max memory allocated: 0.03 GB\n",
      "Max memory reserved: 0.03 GB\n"
     ]
    }
   ],
   "source": [
    "def print_memory_stats():\n",
    "    \"\"\"Print two different measures of GPU memory usage\"\"\"\n",
    "    max_mem_allocated = torch.cuda.max_memory_allocated(device)/1e9\n",
    "    max_mem_reserved = torch.cuda.max_memory_reserved(device)/1e9\n",
    "    print(f\"Max memory allocated: {max_mem_allocated:.2f} GB\")\n",
    "    # reserved (aka 'max_memory_cached') is ~the allocated memory plus pre-cached memory\n",
    "    print(f\"Max memory reserved: {max_mem_reserved:.2f} GB\")\n",
    "\n",
    "    return max_mem_allocated, max_mem_reserved\n",
    "\n",
    "max_mem_allocated, max_mem_reserved = print_memory_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleanup()\n",
    "\n",
    "# model_path = '/NS/llm-1/nobackup/vnanda/llm_base_models/pythia-1b'\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_path).to(device)\n",
    "\n",
    "# print(\"Model loaded\")\n",
    "\n",
    "# batch_size = 1\n",
    "# context_length = 1024\n",
    "\n",
    "# data = torch.randint(0, 10000, (batch_size, context_length), device=device)\n",
    "\n",
    "# output = model(data, labels=data)\n",
    "\n",
    "# output.loss.backward()\n",
    "\n",
    "# # Print memory stats\n",
    "# print_memory_stats()\n",
    "\n",
    "# # Cleanup\n",
    "# del model, data, output\n",
    "# cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '/NS/llm-1/nobackup/vnanda/llm_base_models/pythia-12b'\n",
    "MODEL = model_path.split('/')[-1]\n",
    "measurements = {}\n",
    "BATCH_SIZES = [1,2,4,6,8]\n",
    "CONTEXT_LENGTHS = [1,2,4,6,8,16,32,64,128,256,512,1024,2048]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['memory_measurements_forward_only_pythia-6.9b.json', 'memory_measurements_pythia-1b.json', 'memory_measurements_pythia-70m.json', 'memory_measurements_forward_only_pythia-70m.json', 'memory_measurements_forward_only_pythia-1b.json', 'memory_measurements_pythia-6.9b.json']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "available_measurements = os.listdir('Measurements')\n",
    "\n",
    "print(available_measurements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 1, Context length: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Max memory allocated: 47.60 GB\n",
      "Max memory reserved: 47.62 GB\n",
      "Batch size: 1, Context length: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Max memory allocated: 47.61 GB\n",
      "Max memory reserved: 47.63 GB\n",
      "Batch size: 1, Context length: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Max memory allocated: 47.64 GB\n",
      "Max memory reserved: 47.66 GB\n",
      "Batch size: 1, Context length: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Max memory allocated: 47.67 GB\n",
      "Max memory reserved: 47.68 GB\n",
      "Batch size: 1, Context length: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:05<00:00,  1.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Max memory allocated: 47.69 GB\n",
      "Max memory reserved: 47.71 GB\n",
      "Batch size: 1, Context length: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Max memory allocated: 47.80 GB\n",
      "Max memory reserved: 47.83 GB\n",
      "Batch size: 1, Context length: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Max memory allocated: 48.00 GB\n",
      "Max memory reserved: 48.03 GB\n",
      "Batch size: 1, Context length: 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Max memory allocated: 48.42 GB\n",
      "Max memory reserved: 48.43 GB\n",
      "Batch size: 1, Context length: 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Max memory allocated: 49.25 GB\n",
      "Max memory reserved: 49.28 GB\n",
      "Batch size: 1, Context length: 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Max memory allocated: 50.95 GB\n",
      "Max memory reserved: 50.97 GB\n",
      "Batch size: 1, Context length: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Max memory allocated: 54.24 GB\n",
      "Max memory reserved: 54.28 GB\n",
      "Batch size: 1, Context length: 1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Max memory allocated: 60.90 GB\n",
      "Max memory reserved: 60.96 GB\n",
      "Batch size: 1, Context length: 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Max memory allocated: 74.20 GB\n",
      "Max memory reserved: 74.34 GB\n",
      "Batch size: 2, Context length: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Max memory allocated: 47.61 GB\n",
      "Max memory reserved: 47.63 GB\n",
      "Batch size: 2, Context length: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Max memory allocated: 47.64 GB\n",
      "Max memory reserved: 47.66 GB\n",
      "Batch size: 2, Context length: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Max memory allocated: 47.69 GB\n",
      "Max memory reserved: 47.71 GB\n",
      "Batch size: 2, Context length: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Max memory allocated: 47.75 GB\n",
      "Max memory reserved: 47.77 GB\n",
      "Batch size: 2, Context length: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Max memory allocated: 47.80 GB\n",
      "Max memory reserved: 47.83 GB\n",
      "Batch size: 2, Context length: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Max memory allocated: 48.01 GB\n",
      "Max memory reserved: 48.05 GB\n",
      "Batch size: 2, Context length: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Max memory allocated: 48.43 GB\n",
      "Max memory reserved: 48.44 GB\n",
      "Batch size: 2, Context length: 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Max memory allocated: 49.28 GB\n",
      "Max memory reserved: 49.31 GB\n",
      "Batch size: 2, Context length: 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Max memory allocated: 51.00 GB\n",
      "Max memory reserved: 51.02 GB\n",
      "Batch size: 2, Context length: 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Max memory allocated: 54.34 GB\n",
      "Max memory reserved: 54.39 GB\n",
      "Batch size: 2, Context length: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Max memory allocated: 61.10 GB\n",
      "Max memory reserved: 61.16 GB\n",
      "Batch size: 2, Context length: 1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Max memory allocated: 74.61 GB\n",
      "Max memory reserved: 74.74 GB\n",
      "Batch size: 2, Context length: 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Batch size: 2, Context length: 2048 failed\n",
      "CUDA out of memory. Tried to allocate 20.00 MiB. GPU \n",
      "Batch size: 4, Context length: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Max memory allocated: 47.64 GB\n",
      "Max memory reserved: 47.66 GB\n",
      "Batch size: 4, Context length: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Max memory allocated: 47.69 GB\n",
      "Max memory reserved: 47.71 GB\n",
      "Batch size: 4, Context length: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Max memory allocated: 47.80 GB\n",
      "Max memory reserved: 47.83 GB\n",
      "Batch size: 4, Context length: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Max memory allocated: 47.91 GB\n",
      "Max memory reserved: 47.93 GB\n",
      "Batch size: 4, Context length: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Max memory allocated: 48.01 GB\n",
      "Max memory reserved: 48.03 GB\n",
      "Batch size: 4, Context length: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Max memory allocated: 48.43 GB\n",
      "Max memory reserved: 48.44 GB\n",
      "Batch size: 4, Context length: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Max memory allocated: 49.28 GB\n",
      "Max memory reserved: 49.30 GB\n",
      "Batch size: 4, Context length: 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Max memory allocated: 51.00 GB\n",
      "Max memory reserved: 51.02 GB\n",
      "Batch size: 4, Context length: 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Max memory allocated: 54.34 GB\n",
      "Max memory reserved: 54.39 GB\n",
      "Batch size: 4, Context length: 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Max memory allocated: 61.10 GB\n",
      "Max memory reserved: 61.16 GB\n",
      "Batch size: 4, Context length: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Max memory allocated: 74.60 GB\n",
      "Max memory reserved: 74.74 GB\n",
      "Batch size: 4, Context length: 1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Batch size: 4, Context length: 1024 failed\n",
      "CUDA out of memory. Tried to allocate 20.00 MiB. GPU \n",
      "Batch size: 4, Context length: 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Batch size: 4, Context length: 2048 failed\n",
      "CUDA out of memory. Tried to allocate 640.00 MiB. GPU \n",
      "Batch size: 6, Context length: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Max memory allocated: 47.67 GB\n",
      "Max memory reserved: 47.68 GB\n",
      "Batch size: 6, Context length: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Max memory allocated: 47.74 GB\n",
      "Max memory reserved: 47.77 GB\n",
      "Batch size: 6, Context length: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Max memory allocated: 47.91 GB\n",
      "Max memory reserved: 47.93 GB\n",
      "Batch size: 6, Context length: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Max memory allocated: 48.06 GB\n",
      "Max memory reserved: 48.15 GB\n",
      "Batch size: 6, Context length: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Max memory allocated: 48.22 GB\n",
      "Max memory reserved: 48.28 GB\n",
      "Batch size: 6, Context length: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Max memory allocated: 48.85 GB\n",
      "Max memory reserved: 48.95 GB\n",
      "Batch size: 6, Context length: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:05<00:00,  1.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Max memory allocated: 50.21 GB\n",
      "Max memory reserved: 50.29 GB\n",
      "Batch size: 6, Context length: 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Max memory allocated: 52.65 GB\n",
      "Max memory reserved: 53.32 GB\n",
      "Batch size: 6, Context length: 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Max memory allocated: 57.98 GB\n",
      "Max memory reserved: 58.02 GB\n",
      "Batch size: 6, Context length: 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Max memory allocated: 67.85 GB\n",
      "Max memory reserved: 67.95 GB\n",
      "Batch size: 6, Context length: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:05<00:00,  1.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Batch size: 6, Context length: 512 failed\n",
      "CUDA out of memory. Tried to allocate 60.00 MiB. GPU \n",
      "Batch size: 6, Context length: 1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Batch size: 6, Context length: 1024 failed\n",
      "CUDA out of memory. Tried to allocate 120.00 MiB. GPU \n",
      "Batch size: 6, Context length: 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Batch size: 6, Context length: 2048 failed\n",
      "CUDA out of memory. Tried to allocate 960.00 MiB. GPU \n",
      "Batch size: 8, Context length: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Max memory allocated: 47.69 GB\n",
      "Max memory reserved: 47.71 GB\n",
      "Batch size: 8, Context length: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Max memory allocated: 47.80 GB\n",
      "Max memory reserved: 47.83 GB\n",
      "Batch size: 8, Context length: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Max memory allocated: 48.01 GB\n",
      "Max memory reserved: 48.03 GB\n",
      "Batch size: 8, Context length: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Max memory allocated: 48.22 GB\n",
      "Max memory reserved: 48.28 GB\n",
      "Batch size: 8, Context length: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Max memory allocated: 48.43 GB\n",
      "Max memory reserved: 48.45 GB\n",
      "Batch size: 8, Context length: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Max memory allocated: 49.28 GB\n",
      "Max memory reserved: 49.30 GB\n",
      "Batch size: 8, Context length: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Max memory allocated: 51.00 GB\n",
      "Max memory reserved: 51.02 GB\n",
      "Batch size: 8, Context length: 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Max memory allocated: 54.34 GB\n",
      "Max memory reserved: 54.38 GB\n",
      "Batch size: 8, Context length: 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Max memory allocated: 61.09 GB\n",
      "Max memory reserved: 61.16 GB\n",
      "Batch size: 8, Context length: 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:05<00:00,  1.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Max memory allocated: 74.60 GB\n",
      "Max memory reserved: 74.74 GB\n",
      "Batch size: 8, Context length: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Batch size: 8, Context length: 512 failed\n",
      "CUDA out of memory. Tried to allocate 20.00 MiB. GPU \n",
      "Batch size: 8, Context length: 1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Batch size: 8, Context length: 1024 failed\n",
      "CUDA out of memory. Tried to allocate 640.00 MiB. GPU \n",
      "Batch size: 8, Context length: 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Batch size: 8, Context length: 2048 failed\n",
      "CUDA out of memory. Tried to allocate 1.25 GiB. GPU \n"
     ]
    }
   ],
   "source": [
    "if 'memory_measurements_forward_only_' + MODEL + '.json' in available_measurements:\n",
    "    \n",
    "    with open('Measurements/memory_measurements_forward_only_' + MODEL + '.json', 'r') as f:\n",
    "        measurements = json.load(f)\n",
    "\n",
    "    print(\"Loaded measurements\")\n",
    "    \n",
    "    keys = list(measurements.keys())\n",
    "\n",
    "    # check if all batch sizes are present\n",
    "    for bs in BATCH_SIZES:\n",
    "        if bs not in keys:\n",
    "            print(f\"Batch size {bs} not present\")\n",
    "            continue\n",
    "        for cl in CONTEXT_LENGTHS:\n",
    "            if cl not in measurements[bs]:\n",
    "                print(f\"Context length {cl} not present for batch size {bs}\")\n",
    "                continue\n",
    "\n",
    "else:\n",
    "    for batch_size in BATCH_SIZES:\n",
    "        measurements[batch_size] = {} if batch_size not in measurements else measurements[batch_size]\n",
    "        for context_length in CONTEXT_LENGTHS:\n",
    "\n",
    "            if context_length in measurements[batch_size]:\n",
    "                print(f\"Batch size: {batch_size}, Context length: {context_length} already measured\")\n",
    "                print(f\"Max Memory Allocated: {measurements[batch_size][context_length][0]} GB\")\n",
    "                print(f\"Max Memory Reserved: {measurements[batch_size][context_length][1]} GB\")\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                print(f\"Batch size: {batch_size}, Context length: {context_length}\")\n",
    "                \n",
    "                cleanup()\n",
    "\n",
    "                model = AutoModelForCausalLM.from_pretrained(model_path).to(device)\n",
    "\n",
    "                print(\"Model loaded\")\n",
    "\n",
    "                data = torch.randint(0, 10000, (batch_size, context_length), device=device)\n",
    "\n",
    "                output = model(data, labels=data)\n",
    "\n",
    "                # output.loss.backward()\n",
    "\n",
    "                # Print memory stats\n",
    "                max_mem_allocated, max_mem_reserved = print_memory_stats()\n",
    "\n",
    "                measurements[batch_size][context_length] = (max_mem_allocated, max_mem_reserved)\n",
    "\n",
    "                # Cleanup\n",
    "                del model, data, output\n",
    "                cleanup()\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Batch size: {batch_size}, Context length: {context_length} failed\")\n",
    "                print(e)\n",
    "                measurements[batch_size][context_length] = (100, 100)\n",
    "                try:\n",
    "                    del model\n",
    "                except:\n",
    "                    pass\n",
    "                try:\n",
    "                    del data\n",
    "                except:\n",
    "                    pass\n",
    "                try:\n",
    "                    del output\n",
    "                except:\n",
    "                    pass\n",
    "                cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: {1: (47.60166912, 47.620030464),\n",
       "  2: (47.61424384, 47.632613376),\n",
       "  4: (47.639765504, 47.6577792),\n",
       "  6: (47.665762816, 47.682945024),\n",
       "  8: (47.691723264, 47.714402304),\n",
       "  16: (47.79668736, 47.82555136),\n",
       "  32: (48.00386304, 48.033169408),\n",
       "  64: (48.4203648, 48.43372544),\n",
       "  128: (49.252058112, 49.280974848),\n",
       "  256: (50.9524224, 50.97127936),\n",
       "  512: (54.24244224, 54.282682368),\n",
       "  1024: (60.89544192, 60.955820032),\n",
       "  2048: (74.203132928, 74.337746944)},\n",
       " 2: {1: (47.61442816, 47.632613376),\n",
       "  2: (47.640153088, 47.659876352),\n",
       "  4: (47.692884992, 47.714402304),\n",
       "  6: (47.74565376, 47.766831104),\n",
       "  8: (47.79943424, 47.82555136),\n",
       "  16: (48.009780224, 48.054140928),\n",
       "  32: (48.432462848, 48.4442112),\n",
       "  64: (49.276813312, 49.306140672),\n",
       "  128: (51.0036736, 51.021611008),\n",
       "  256: (54.343287808, 54.385442816),\n",
       "  512: (61.098349568, 61.159243776),\n",
       "  1024: (74.608948224, 74.74249728),\n",
       "  2048: (100, 100)},\n",
       " 4: {1: (47.63994624, 47.6577792),\n",
       "  2: (47.692442624, 47.714402304),\n",
       "  4: (47.798955008, 47.82555136),\n",
       "  6: (47.90642176, 47.93040896),\n",
       "  8: (48.009772032, 48.033169408),\n",
       "  16: (48.432684032, 48.4442112),\n",
       "  32: (49.275756544, 49.301946368),\n",
       "  64: (51.000413184, 51.021611008),\n",
       "  128: (54.341297152, 54.385442816),\n",
       "  256: (61.095990272, 61.157146624),\n",
       "  512: (74.604229632, 74.738302976),\n",
       "  1024: (100, 100),\n",
       "  2048: (100, 100)},\n",
       " 6: {1: (47.665633792, 47.682945024),\n",
       "  2: (47.74473216, 47.766831104),\n",
       "  4: (47.905942528, 47.93040896),\n",
       "  6: (48.064011264, 48.15060992),\n",
       "  8: (48.221715456, 48.27643904),\n",
       "  16: (48.854515712, 48.94752768),\n",
       "  32: (50.209695744, 50.293899264),\n",
       "  64: (52.653570048, 53.322186752),\n",
       "  128: (57.98258176, 58.024001536),\n",
       "  256: (67.847881728, 67.949821952),\n",
       "  512: (100, 100),\n",
       "  1024: (100, 100),\n",
       "  2048: (100, 100)},\n",
       " 8: {1: (47.691557376, 47.714402304),\n",
       "  2: (47.798447104, 47.82555136),\n",
       "  4: (48.008342528, 48.033169408),\n",
       "  6: (48.221273088, 48.27643904),\n",
       "  8: (48.430889984, 48.446308352),\n",
       "  16: (49.276346368, 49.301946368),\n",
       "  32: (50.99859456, 51.017416704),\n",
       "  64: (54.340232192, 54.37915136),\n",
       "  128: (61.091566592, 61.155049472),\n",
       "  256: (74.598626304, 74.736205824),\n",
       "  512: (100, 100),\n",
       "  1024: (100, 100),\n",
       "  2048: (100, 100)}}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump measurements\n",
    "import json\n",
    "\n",
    "with open(f\"Measurements/memory_measurements_forward_only_{MODEL}.json\", 'w') as f:\n",
    "    json.dump(measurements, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
