{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference: \n",
    "- https://www.youtube.com/watch?v=gXDsVcY8TXQ&t=3146s\n",
    "- https://github.com/TrelisResearch/install-guides/blob/main/multi-gpu/test_scripts/test_fsdp.py\n",
    "- https://github.com/huggingface/trl/issues/1303"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB__SERVICE_WAIT\"] = \"300\"\n",
    "os.environ[\"HF_HOME\"] = \"/NS/llm-1/nobackup/afkhan/HF_CACHE/Misc\"\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = \"/NS/llm-1/nobackup/afkhan/HF_CACHE/Datasets\"\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"/NS/llm-1/nobackup/afkhan/HF_CACHE/Models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = os.getenv(\"TRANSFORMERS_CACHE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers datasets bitsandbytes deepspeed accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from datasets import load_dataset\n",
    "from accelerate import PartialState, notebook_launcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install wandb\n",
    "import wandb\n",
    "# from utils import print_trainable_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fsdp_trainer():\n",
    "\n",
    "    DEVICE_MAP = 'FSDP'\n",
    "\n",
    "    model_name = 'Llama-2-7b-hf'\n",
    "    model_path = \"/NS/llm-1/nobackup/vnanda/llm_base_models/Llama-2-7b-hf\"\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path, cache_dir=cache_dir\n",
    "    )\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, cache_dir=cache_dir)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    ds_name = \"weyaxi--sci-datasets\"\n",
    "    ds = load_dataset(\"Weyaxi/sci-datasets\", \"alpaca\")\n",
    "\n",
    "    # Keep only 100 examples for now\n",
    "    ds['train'] = ds['train'].select(range(100))\n",
    "\n",
    "    def merge_columns(example):\n",
    "        example['text'] = '### Instruction: ' + example['instruction'] + ' ### Answer: ' + example['output']\n",
    "        return example\n",
    "\n",
    "    ds['train'] = ds['train'].map(merge_columns)\n",
    "\n",
    "    ds = ds.map(\n",
    "        lambda samples: tokenizer(samples[\"text\"]), batched=True,\n",
    "    ) \n",
    "\n",
    "    ## Wandb Related\n",
    "\n",
    "    WANDB_PROJECT = \"FSDP-Analysis\"\n",
    "    WANDB_RUN_NAME = f\"{model_name}-{ds_name}-full-finetune\" + \"-fsdp\"\n",
    "\n",
    "    ## Logging Related\n",
    "\n",
    "    REPORT_TO = \"wandb\"\n",
    "    OUTPUT_DIR = f\"./output/{model_name}-{ds_name}-full-finetune\" + \"-fsdp\"\n",
    "    LOGGING_DIR = f\"./logs/{model_name}-{ds_name}-full-finetune\" + \"-fsdp\"\n",
    "    LOGGING_STRATEGY = \"steps\"\n",
    "    LOGGING_STEPS = 10\n",
    "\n",
    "    ## Training Duration Related\n",
    "\n",
    "    MAX_STEPS = 1000\n",
    "\n",
    "    ## Optimizer Related\n",
    "\n",
    "    LEARNING_RATE = 2e-4\n",
    "    LR_SCHEDULER_TYPE = \"linear\"\n",
    "    WARMUP_RATIO = 0.1\n",
    "\n",
    "    ## Batch Related\n",
    "\n",
    "    PER_DEVICE_TRAIN_BATCH_SIZE = 8\n",
    "    PER_DEVICE_EVAL_BATCH_SIZE = 8\n",
    "\n",
    "    ## Gradient Related (Also related to Parallelism)\n",
    "\n",
    "    GRADIENT_CHECKPOINTING = True\n",
    "    # Use reentrant starts a more efficient method of recomputing the graph from checkpoints\n",
    "    USE_REENTRANT = True # Set False for DDP and True for Model/Pipeline Parallelism\n",
    "\n",
    "    # Configure Wandb project and run\n",
    "\n",
    "    wandb.init(project=WANDB_PROJECT, name=WANDB_RUN_NAME)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        # Logging Related\n",
    "        report_to=REPORT_TO,\n",
    "        output_dir = OUTPUT_DIR,\n",
    "        logging_dir = LOGGING_DIR,\n",
    "        logging_strategy = LOGGING_STRATEGY,\n",
    "        logging_steps = LOGGING_STEPS,\n",
    "        # Training Duration Related\n",
    "        max_steps = MAX_STEPS,\n",
    "        # Optimizer Related\n",
    "        learning_rate = LEARNING_RATE,\n",
    "        lr_scheduler_type = LR_SCHEDULER_TYPE,\n",
    "        warmup_ratio = WARMUP_RATIO,\n",
    "        # Batch Related\n",
    "        per_device_train_batch_size = PER_DEVICE_TRAIN_BATCH_SIZE,\n",
    "        per_device_eval_batch_size = PER_DEVICE_EVAL_BATCH_SIZE,\n",
    "        # Gradient Related\n",
    "        gradient_checkpointing = GRADIENT_CHECKPOINTING,\n",
    "        gradient_checkpointing_kwargs = {\"use_reentrant\": USE_REENTRANT},\n",
    "    )\n",
    "\n",
    "    wandb.config.update(training_args)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n",
    "        train_dataset=ds['train'],\n",
    "        eval_dataset=ds['train'],\n",
    "    )\n",
    "\n",
    "    # if getattr(trainer.accelerator.state, \"fsdp_plugin\", None):\n",
    "    #     from peft.utils.other import fsdp_auto_wrap_policy\n",
    "\n",
    "    #     fsdp_plugin = trainer.accelerator.state.fsdp_plugin\n",
    "    #     fsdp_plugin.auto_wrap_policy = fsdp_auto_wrap_policy(trainer.model)\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    # Save Model\n",
    "    model.save_pretrained(f'Saves/{model_name}-{ds_name}-full-finetune' + \"-fsdp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.77s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.78s/it]\n",
      "Map: 100%|██████████| 100/100 [00:01<00:00, 89.98 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 237.46 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 234.68 examples/s]\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maflah\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maflah\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/NS/llm-1/work/afkhan/FSDP_Experiments/wandb/run-20240716_143708-x0993yvr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/NS/llm-1/work/afkhan/FSDP_Experiments/wandb/run-20240716_143708-tkcs2gg5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/aflah/FSDP-Analysis/runs/x0993yvr' target=\"_blank\">Llama-2-7b-hf-weyaxi--sci-datasets-full-finetune-fsdp</a></strong> to <a href='https://wandb.ai/aflah/FSDP-Analysis' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/aflah/FSDP-Analysis' target=\"_blank\">https://wandb.ai/aflah/FSDP-Analysis</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/aflah/FSDP-Analysis/runs/tkcs2gg5' target=\"_blank\">Llama-2-7b-hf-weyaxi--sci-datasets-full-finetune-fsdp</a></strong> to <a href='https://wandb.ai/aflah/FSDP-Analysis' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/aflah/FSDP-Analysis/runs/x0993yvr' target=\"_blank\">https://wandb.ai/aflah/FSDP-Analysis/runs/x0993yvr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/aflah/FSDP-Analysis' target=\"_blank\">https://wandb.ai/aflah/FSDP-Analysis</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/aflah/FSDP-Analysis/runs/tkcs2gg5' target=\"_blank\">https://wandb.ai/aflah/FSDP-Analysis/runs/tkcs2gg5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-07-16 14:37:23,906] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Warning: The default cache directory for DeepSpeed Triton autotune, /home/afkhan/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.\n",
      "[2024-07-16 14:37:25,038] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/NS/llm-1/nobackup/afkhan/anaconda3/envs/fsdp_env/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: The default cache directory for DeepSpeed Triton autotune, /home/afkhan/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/NS/llm-1/nobackup/afkhan/anaconda3/envs/fsdp_env/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3\n",
      "\u001b[93m [WARNING] \u001b[0m using untested triton version (2.3.1), only 1.0.0 is known to be compatible\n",
      "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3\n",
      "\u001b[93m [WARNING] \u001b[0m using untested triton version (2.3.1), only 1.0.0 is known to be compatible\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "W0716 14:37:37.204000 140358814619456 torch/multiprocessing/spawn.py:145] Terminating process 3788212 via signal SIGTERM\n",
      "E0716 14:37:37.347000 140358814619456 torch/distributed/elastic/multiprocessing/api.py:695] failed (exitcode: 1) local_rank: 0 (pid: 3788210) of fn: train_fsdp_trainer (start_method: fork)\n",
      "E0716 14:37:37.347000 140358814619456 torch/distributed/elastic/multiprocessing/api.py:695] Traceback (most recent call last):\n",
      "E0716 14:37:37.347000 140358814619456 torch/distributed/elastic/multiprocessing/api.py:695]   File \"/NS/llm-1/nobackup/afkhan/anaconda3/envs/fsdp_env/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py\", line 656, in _poll\n",
      "E0716 14:37:37.347000 140358814619456 torch/distributed/elastic/multiprocessing/api.py:695]     self._pc.join(-1)\n",
      "E0716 14:37:37.347000 140358814619456 torch/distributed/elastic/multiprocessing/api.py:695]   File \"/NS/llm-1/nobackup/afkhan/anaconda3/envs/fsdp_env/lib/python3.10/site-packages/torch/multiprocessing/spawn.py\", line 188, in join\n",
      "E0716 14:37:37.347000 140358814619456 torch/distributed/elastic/multiprocessing/api.py:695]     raise ProcessRaisedException(msg, error_index, failed_process.pid)\n",
      "E0716 14:37:37.347000 140358814619456 torch/distributed/elastic/multiprocessing/api.py:695] torch.multiprocessing.spawn.ProcessRaisedException: \n",
      "E0716 14:37:37.347000 140358814619456 torch/distributed/elastic/multiprocessing/api.py:695] \n",
      "E0716 14:37:37.347000 140358814619456 torch/distributed/elastic/multiprocessing/api.py:695] -- Process 0 terminated with the following error:\n",
      "E0716 14:37:37.347000 140358814619456 torch/distributed/elastic/multiprocessing/api.py:695] Traceback (most recent call last):\n",
      "E0716 14:37:37.347000 140358814619456 torch/distributed/elastic/multiprocessing/api.py:695]   File \"/NS/llm-1/nobackup/afkhan/anaconda3/envs/fsdp_env/lib/python3.10/site-packages/torch/multiprocessing/spawn.py\", line 75, in _wrap\n",
      "E0716 14:37:37.347000 140358814619456 torch/distributed/elastic/multiprocessing/api.py:695]     fn(i, *args)\n",
      "E0716 14:37:37.347000 140358814619456 torch/distributed/elastic/multiprocessing/api.py:695]   File \"/NS/llm-1/nobackup/afkhan/anaconda3/envs/fsdp_env/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py\", line 580, in _wrap\n",
      "E0716 14:37:37.347000 140358814619456 torch/distributed/elastic/multiprocessing/api.py:695]     ret = record(fn)(*args_)\n",
      "E0716 14:37:37.347000 140358814619456 torch/distributed/elastic/multiprocessing/api.py:695]   File \"/NS/llm-1/nobackup/afkhan/anaconda3/envs/fsdp_env/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 347, in wrapper\n",
      "E0716 14:37:37.347000 140358814619456 torch/distributed/elastic/multiprocessing/api.py:695]     return f(*args, **kwargs)\n",
      "E0716 14:37:37.347000 140358814619456 torch/distributed/elastic/multiprocessing/api.py:695]   File \"/tmp/ipykernel_3787111/2977254477.py\", line 106, in train_fsdp_trainer\n",
      "E0716 14:37:37.347000 140358814619456 torch/distributed/elastic/multiprocessing/api.py:695]     trainer.train()\n",
      "E0716 14:37:37.347000 140358814619456 torch/distributed/elastic/multiprocessing/api.py:695]   File \"/NS/llm-1/nobackup/afkhan/anaconda3/envs/fsdp_env/lib/python3.10/site-packages/transformers/trainer.py\", line 1932, in train\n",
      "E0716 14:37:37.347000 140358814619456 torch/distributed/elastic/multiprocessing/api.py:695]     return inner_training_loop(\n",
      "E0716 14:37:37.347000 140358814619456 torch/distributed/elastic/multiprocessing/api.py:695]   File \"/NS/llm-1/nobackup/afkhan/anaconda3/envs/fsdp_env/lib/python3.10/site-packages/transformers/trainer.py\", line 2330, in _inner_training_loop\n",
      "E0716 14:37:37.347000 140358814619456 torch/distributed/elastic/multiprocessing/api.py:695]     self.optimizer.step()\n",
      "E0716 14:37:37.347000 140358814619456 torch/distributed/elastic/multiprocessing/api.py:695]   File \"/NS/llm-1/nobackup/afkhan/anaconda3/envs/fsdp_env/lib/python3.10/site-packages/accelerate/optimizer.py\", line 170, in step\n",
      "E0716 14:37:37.347000 140358814619456 torch/distributed/elastic/multiprocessing/api.py:695]     self.optimizer.step(closure)\n",
      "E0716 14:37:37.347000 140358814619456 torch/distributed/elastic/multiprocessing/api.py:695]   File \"/NS/llm-1/nobackup/afkhan/anaconda3/envs/fsdp_env/lib/python3.10/site-packages/torch/optim/lr_scheduler.py\", line 75, in wrapper\n",
      "E0716 14:37:37.347000 140358814619456 torch/distributed/elastic/multiprocessing/api.py:695]     return wrapped(*args, **kwargs)\n",
      "E0716 14:37:37.347000 140358814619456 torch/distributed/elastic/multiprocessing/api.py:695]   File \"/NS/llm-1/nobackup/afkhan/anaconda3/envs/fsdp_env/lib/python3.10/site-packages/torch/optim/optimizer.py\", line 391, in wrapper\n",
      "E0716 14:37:37.347000 140358814619456 torch/distributed/elastic/multiprocessing/api.py:695]     out = func(*args, **kwargs)\n",
      "E0716 14:37:37.347000 140358814619456 torch/distributed/elastic/multiprocessing/api.py:695]   File \"/NS/llm-1/nobackup/afkhan/anaconda3/envs/fsdp_env/lib/python3.10/site-packages/torch/optim/optimizer.py\", line 76, in _use_grad\n",
      "E0716 14:37:37.347000 140358814619456 torch/distributed/elastic/multiprocessing/api.py:695]     ret = func(self, *args, **kwargs)\n",
      "E0716 14:37:37.347000 140358814619456 torch/distributed/elastic/multiprocessing/api.py:695]   File \"/NS/llm-1/nobackup/afkhan/anaconda3/envs/fsdp_env/lib/python3.10/site-packages/torch/optim/adamw.py\", line 177, in step\n",
      "E0716 14:37:37.347000 140358814619456 torch/distributed/elastic/multiprocessing/api.py:695]     has_complex = self._init_group(\n",
      "E0716 14:37:37.347000 140358814619456 torch/distributed/elastic/multiprocessing/api.py:695]   File \"/NS/llm-1/nobackup/afkhan/anaconda3/envs/fsdp_env/lib/python3.10/site-packages/torch/optim/adamw.py\", line 124, in _init_group\n",
      "E0716 14:37:37.347000 140358814619456 torch/distributed/elastic/multiprocessing/api.py:695]     state[\"exp_avg\"] = torch.zeros_like(\n",
      "E0716 14:37:37.347000 140358814619456 torch/distributed/elastic/multiprocessing/api.py:695] torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU \n",
      "E0716 14:37:37.347000 140358814619456 torch/distributed/elastic/multiprocessing/api.py:695] \n"
     ]
    },
    {
     "ename": "ChildFailedError",
     "evalue": "\n============================================================\ntrain_fsdp_trainer FAILED\n------------------------------------------------------------\nFailures:\n  <NO_OTHER_FAILURES>\n------------------------------------------------------------\nRoot Cause (first observed failure):\n[0]:\n  time      : 2024-07-16_14:37:36\n  host      : sws-2h100-02.mpi-sws.org\n  rank      : 0 (local_rank: 0)\n  exitcode  : 1 (pid: 3788210)\n  error_file: /tmp/torchelastic_qhh0qw_e/none_8bger7wv/attempt_0/0/error.json\n  traceback : Traceback (most recent call last):\n    File \"/NS/llm-1/nobackup/afkhan/anaconda3/envs/fsdp_env/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 347, in wrapper\n      return f(*args, **kwargs)\n    File \"/tmp/ipykernel_3787111/2977254477.py\", line 106, in train_fsdp_trainer\n      trainer.train()\n    File \"/NS/llm-1/nobackup/afkhan/anaconda3/envs/fsdp_env/lib/python3.10/site-packages/transformers/trainer.py\", line 1932, in train\n      return inner_training_loop(\n    File \"/NS/llm-1/nobackup/afkhan/anaconda3/envs/fsdp_env/lib/python3.10/site-packages/transformers/trainer.py\", line 2330, in _inner_training_loop\n      self.optimizer.step()\n    File \"/NS/llm-1/nobackup/afkhan/anaconda3/envs/fsdp_env/lib/python3.10/site-packages/accelerate/optimizer.py\", line 170, in step\n      self.optimizer.step(closure)\n    File \"/NS/llm-1/nobackup/afkhan/anaconda3/envs/fsdp_env/lib/python3.10/site-packages/torch/optim/lr_scheduler.py\", line 75, in wrapper\n      return wrapped(*args, **kwargs)\n    File \"/NS/llm-1/nobackup/afkhan/anaconda3/envs/fsdp_env/lib/python3.10/site-packages/torch/optim/optimizer.py\", line 391, in wrapper\n      out = func(*args, **kwargs)\n    File \"/NS/llm-1/nobackup/afkhan/anaconda3/envs/fsdp_env/lib/python3.10/site-packages/torch/optim/optimizer.py\", line 76, in _use_grad\n      ret = func(self, *args, **kwargs)\n    File \"/NS/llm-1/nobackup/afkhan/anaconda3/envs/fsdp_env/lib/python3.10/site-packages/torch/optim/adamw.py\", line 177, in step\n      has_complex = self._init_group(\n    File \"/NS/llm-1/nobackup/afkhan/anaconda3/envs/fsdp_env/lib/python3.10/site-packages/torch/optim/adamw.py\", line 124, in _init_group\n      state[\"exp_avg\"] = torch.zeros_like(\n  torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU \n  \n============================================================",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mChildFailedError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mnotebook_launcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_fsdp_trainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_processes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/NS/llm-1/nobackup/afkhan/anaconda3/envs/fsdp_env/lib/python3.10/site-packages/accelerate/launchers.py:245\u001b[0m, in \u001b[0;36mnotebook_launcher\u001b[0;34m(function, args, num_processes, mixed_precision, use_port, master_addr, node_rank, num_nodes, rdzv_backend, rdzv_endpoint, rdzv_conf, rdzv_id, max_restarts, monitor_interval, log_line_prefix_template)\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_version(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>=\u001b[39m\u001b[38;5;124m\"\u001b[39m, ELASTIC_LOG_LINE_PREFIX_TEMPLATE_PYTORCH_VERSION):\n\u001b[1;32m    244\u001b[0m         launch_config_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlog_line_prefix_template\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m log_line_prefix_template\n\u001b[0;32m--> 245\u001b[0m     \u001b[43melastic_launch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLaunchConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlaunch_config_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mentrypoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ProcessRaisedException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m e\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[0;32m/NS/llm-1/nobackup/afkhan/anaconda3/envs/fsdp_env/lib/python3.10/site-packages/torch/distributed/launcher/api.py:132\u001b[0m, in \u001b[0;36melastic_launch.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m--> 132\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlaunch_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_entrypoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/NS/llm-1/nobackup/afkhan/anaconda3/envs/fsdp_env/lib/python3.10/site-packages/torch/distributed/launcher/api.py:263\u001b[0m, in \u001b[0;36mlaunch_agent\u001b[0;34m(config, entrypoint, args)\u001b[0m\n\u001b[1;32m    256\u001b[0m     events\u001b[38;5;241m.\u001b[39mrecord(agent\u001b[38;5;241m.\u001b[39mget_event_succeeded())\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mis_failed():\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;66;03m# ChildFailedError is treated specially by @record\u001b[39;00m\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;66;03m# if the error files for the failed children exist\u001b[39;00m\n\u001b[1;32m    261\u001b[0m         \u001b[38;5;66;03m# @record will copy the first error (root cause)\u001b[39;00m\n\u001b[1;32m    262\u001b[0m         \u001b[38;5;66;03m# to the error file of the launcher process.\u001b[39;00m\n\u001b[0;32m--> 263\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChildFailedError(\n\u001b[1;32m    264\u001b[0m             name\u001b[38;5;241m=\u001b[39mentrypoint_name,\n\u001b[1;32m    265\u001b[0m             failures\u001b[38;5;241m=\u001b[39mresult\u001b[38;5;241m.\u001b[39mfailures,\n\u001b[1;32m    266\u001b[0m         )\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\u001b[38;5;241m.\u001b[39mreturn_values\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ChildFailedError:\n",
      "\u001b[0;31mChildFailedError\u001b[0m: \n============================================================\ntrain_fsdp_trainer FAILED\n------------------------------------------------------------\nFailures:\n  <NO_OTHER_FAILURES>\n------------------------------------------------------------\nRoot Cause (first observed failure):\n[0]:\n  time      : 2024-07-16_14:37:36\n  host      : sws-2h100-02.mpi-sws.org\n  rank      : 0 (local_rank: 0)\n  exitcode  : 1 (pid: 3788210)\n  error_file: /tmp/torchelastic_qhh0qw_e/none_8bger7wv/attempt_0/0/error.json\n  traceback : Traceback (most recent call last):\n    File \"/NS/llm-1/nobackup/afkhan/anaconda3/envs/fsdp_env/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 347, in wrapper\n      return f(*args, **kwargs)\n    File \"/tmp/ipykernel_3787111/2977254477.py\", line 106, in train_fsdp_trainer\n      trainer.train()\n    File \"/NS/llm-1/nobackup/afkhan/anaconda3/envs/fsdp_env/lib/python3.10/site-packages/transformers/trainer.py\", line 1932, in train\n      return inner_training_loop(\n    File \"/NS/llm-1/nobackup/afkhan/anaconda3/envs/fsdp_env/lib/python3.10/site-packages/transformers/trainer.py\", line 2330, in _inner_training_loop\n      self.optimizer.step()\n    File \"/NS/llm-1/nobackup/afkhan/anaconda3/envs/fsdp_env/lib/python3.10/site-packages/accelerate/optimizer.py\", line 170, in step\n      self.optimizer.step(closure)\n    File \"/NS/llm-1/nobackup/afkhan/anaconda3/envs/fsdp_env/lib/python3.10/site-packages/torch/optim/lr_scheduler.py\", line 75, in wrapper\n      return wrapped(*args, **kwargs)\n    File \"/NS/llm-1/nobackup/afkhan/anaconda3/envs/fsdp_env/lib/python3.10/site-packages/torch/optim/optimizer.py\", line 391, in wrapper\n      out = func(*args, **kwargs)\n    File \"/NS/llm-1/nobackup/afkhan/anaconda3/envs/fsdp_env/lib/python3.10/site-packages/torch/optim/optimizer.py\", line 76, in _use_grad\n      ret = func(self, *args, **kwargs)\n    File \"/NS/llm-1/nobackup/afkhan/anaconda3/envs/fsdp_env/lib/python3.10/site-packages/torch/optim/adamw.py\", line 177, in step\n      has_complex = self._init_group(\n    File \"/NS/llm-1/nobackup/afkhan/anaconda3/envs/fsdp_env/lib/python3.10/site-packages/torch/optim/adamw.py\", line 124, in _init_group\n      state[\"exp_avg\"] = torch.zeros_like(\n  torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU \n  \n============================================================"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/NS/llm-1/nobackup/afkhan/anaconda3/envs/fsdp_env/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/NS/llm-1/nobackup/afkhan/anaconda3/envs/fsdp_env/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/NS/llm-1/nobackup/afkhan/anaconda3/envs/fsdp_env/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/NS/llm-1/nobackup/afkhan/anaconda3/envs/fsdp_env/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/NS/llm-1/nobackup/afkhan/anaconda3/envs/fsdp_env/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/NS/llm-1/nobackup/afkhan/anaconda3/envs/fsdp_env/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/NS/llm-1/nobackup/afkhan/anaconda3/envs/fsdp_env/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/NS/llm-1/nobackup/afkhan/anaconda3/envs/fsdp_env/lib/python3.10/asyncio/base_events.py\", line 1871, in _run_once\n",
      "    event_list = self._selector.select(timeout)\n",
      "  File \"/NS/llm-1/nobackup/afkhan/anaconda3/envs/fsdp_env/lib/python3.10/selectors.py\", line 469, in select\n",
      "    fd_event_list = self._selector.poll(timeout, max_ev)\n",
      "  File \"/NS/llm-1/nobackup/afkhan/anaconda3/envs/fsdp_env/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py\", line 76, in _terminate_process_handler\n",
      "    raise SignalException(f\"Process {os.getpid()} got signal: {sigval}\", sigval=sigval)\n",
      "torch.distributed.elastic.multiprocessing.api.SignalException: Process 3787111 got signal: 15\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "notebook_launcher(train_fsdp_trainer, args=(), num_processes=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "peft_mem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
